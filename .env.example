# College Buddy - Environment Configuration
# Copy this to .env and update with your values

# ============================================
# GEMINI API (Fallback / Optional)
# ============================================
GEMINI_API_KEY=your_gemini_api_key_here

# ============================================
# LOCAL MODEL (Ollama) - Primary
# ============================================
# Enable/disable local model
USE_LOCAL_MODEL=true

# Ollama server URL (default for local installation)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (options: gemma3:2b, gemma3:4b, gemma3:7b, gemma2:4b, deepseek-r1:8b)
LOCAL_MODEL_NAME=gemma3:4b

# Timeout before falling back to Gemini (seconds)
# Adjust based on your CPU performance:
# - Fast CPU (i7/i9): 10-12 seconds
# - Medium CPU (i5): 15 seconds
# - Slow CPU (i3/old): 20-25 seconds
LOCAL_MODEL_TIMEOUT=15

# ============================================
# DEPLOYMENT STRATEGY
# ============================================
# Recommended configurations:

# OPTION 1: Hybrid (Recommended)
# - Cost savings + reliability
# USE_LOCAL_MODEL=true
# GEMINI_API_KEY=your_key_here
# LOCAL_MODEL_TIMEOUT=15

# OPTION 2: Local Only (Maximum Privacy)
# - Free, private, but slower
# USE_LOCAL_MODEL=true
# Remove/comment GEMINI_API_KEY line

# OPTION 3: Gemini Only (Maximum Reliability)
# - Fast, high quality, but costs money
# USE_LOCAL_MODEL=false
# GEMINI_API_KEY=your_key_here

# ============================================
# OTHER SETTINGS (from your existing setup)
# ============================================
# Add any other environment variables your app needs here
